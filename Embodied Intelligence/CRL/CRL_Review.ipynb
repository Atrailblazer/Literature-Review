{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Curious Representation Learning for Embodied Intelligence\n",
    "## 文献出处及作者\n",
    "* 会议：ICCV 2001\n",
    "* 作者<br>\n",
    "Yilun Du - MIT<br>\n",
    "Chuang Gan -MIT-IBM Watson AI Lab<br>\n",
    "Phillip Isola - MIT\n",
    "## 文献主要内容\n",
    "* 目标：解决具身智能中的一个关键问题：如何让智能体在没有任何外部任务或监督标签的情况下，通过主动与环境交互来学习通用、任务无关的视觉表征。\n",
    "* 核心思想：提出了一个名为 好奇表征学习（Curious Representation Learning, CRL） 的统一框架。该框架通过一个极小极大博弈 同时训练一个强化学习探索策略和一个自监督视觉表征模型：<br>\n",
    "**策略目标：** 探索环境，寻找那些能让表征模型产生高预测误差的观测图像。策略的奖励被设定为表征模型的损失。<br>\n",
    "**模型目标：** 努力从策略提供的“困难”图像中学习，不断降低其损失，从而变得更加强大和鲁棒。<br>\n",
    "这个博弈过程形成了一个良性循环：策略为了获得高奖励，会不断寻找模型不熟悉的、新颖的图像，从而为模型提供了持续且有挑战性的数据；而模型通过在这些数据上学习，其表征能力不断增强，又迫使策略去探索更具挑战性的区域。\n",
    "## 关键贡献：\n",
    "1. 提出了CRL框架，将好奇心探索与自监督表征学习有机结合\n",
    "2. 证明了所学表征能有效提升多种下游具身任务（如导航）的性能，并且在迁移到下游任务时必须冻结视觉编码器以防止性能下降。\n",
    "3. 展示了尽管仅在仿真环境中训练，所学表征能够迁移到真实世界图像的理解上，并产生可解释的结果。\n",
    "## 好奇表征学习CRL\n",
    "* 目标：展示了尽管仅在仿真环境中训练，所学表征能够迁移到真实世界图像的理解上，并产生可解释的结果。\n",
    "* 代码：https://yilundu.github.io/crl/\n",
    "### 对比表示学习\n",
    "* 目标：学习到一个视觉编码器$M_\\phi$，它能够将图像映射到一个特征空间，使得同一张图像的不同增强版本（“正样本对”）在特征空间中非常接近，而不同图像的增强版本（“负样本对”）则相距甚远，这样学习到的表征不依赖于人工标签，能够捕捉图像的本质特征\n",
    "* 关键组件：<br>\n",
    "**表征模型$M_\\phi$：** ResNet50，从图像中提取特征<br>\n",
    "**投影头$g_\\psi$：** 一个2层的MLP，将ResNet提取的特征映射到一个更适合做对比学习的低维空间。<br>\n",
    "**一组数据增强$\\tau$：** 定义了一系列随机的图像变换，包括：水平翻转、随机缩放裁剪、颜色饱和度变化。这些增强用于创造“正样本对”。\n",
    "* 工作流程：<br>\n",
    "**1.** 对于一个包含N张图像的批次${x_k}$<br>\n",
    "**2.** 对批次中的每一张图像$x_i$，从$\\tau$中独立采样两次，得到两个不同的增强版本$\\tilde{x}_i^1$和$\\tilde{x}_i^2$。构成N个正样本对。<br>\n",
    "**3.** 将2N张增强后的图片依次通过$M_\\phi$和$g_\\psi$。<br>\n",
    "**4.** 对投影头的输出进行L2归一化，得到最终的潜向量$z$。\n",
    "* 损失函数：<br>\n",
    "使用**InfoNCE**作为损失函数：<br>\n",
    "$$L_contrast = -\\frac{1}{N} \\sum^N_{i=1}log\\frac{exp(sim(\\tilde{z}^1_i,\\tilde{z}^2_i)/\\tau)}{\\sum^N_{j=1}\\sum^N_{k=1}exp(sim(\\tilde{z}^1_j,\\tilde{z}^2_k)/\\tau))}$$<br>\n",
    "分子：衡量正样本对$(\\tilde{z}^1_i,\\tilde{z}^2_i)$的相似度，鼓励它越大越好<br>\n",
    "分母：衡量一个正样本对与批次中所有其他样本的相似度<br>\n",
    "参数$\\tau$：用于调节分布的尖锐程度\n",
    "### 基于内在特征的表征学习（核心创新点）\n",
    "1. 从静态数据到交互环境的范式转变<br>\n",
    "**传统：** 模型被动的从固定数据集$p_data$中学习。目标是$min_\\phi E_{x\\sim p_{data}}[L_rep]$<br>\n",
    "**CRL：** 数据由策略$\\pi_\\theta$主动选择，策略的目标是最大化累积奖励：$max_\\theta E_{x\\sim \\pi_\\theta}[\\sum r_t]$\n",
    "2. 好奇心奖励<br>\n",
    "CRL将表征模型的损失函数直接作为策略的奖励<br>\n",
    "在每一个时间步t，策略获得的奖励$r_t=L_{rep}(M_\\phi,x_t)$<br>，因此，策略的优化目标为：\n",
    "$$\\underset{\\theta}{max}E_{x\\sim \\pi_\\theta}[\\sum^T_{t=0}L_{rep}(M_\\phi,x)]$$\n",
    "3. 极小极大博弈<br>\n",
    "将表征模型的目标和策略的目标集合起来，就得到了CRL的总体目标函数：<br>\n",
    "$$\\underset{\\theta}{max}\\underset{\\phi}{min}E_{x\\sim \\pi_\\theta}[\\sum^T_{t=0}L_{rep}(M_\\phi,x)]$$<br>\n",
    "**策略$\\pi_\\theta$：** 试图找到让模型$M_\\phi$损失最大的数据<br>\n",
    "**模型$M_\\phi$：** 努力从策略提供的困难数据中学习，最小化自己的损失<br>\n",
    "这个博弈促使策略不断探索新区域，同时模型也变得更强，鲁棒性更高\n",
    "4. 和以往好奇心方法的差异\n",
    "**以往的方法：** 需要手动设计一个预测任务定义“新奇性”\n",
    "**CRL：** 提供了一个通用框架，任何先进的自监督表征学习算法都可以直接插入作为好奇心目标\n",
    "### 模型与策略优化\n",
    "1. 奖励函数的修正<br>\n",
    "**问题：** 直接使用完整的对比损失$L_contrast$作为奖励，策略会发现一个**作弊方法**：让智能体静止不动。因为所有观测图像都相同，负样本对的相似度会极高，导致分母巨大，从而使损失值$L_contrast$变得很大，策略不探索就能获得高奖励<br>\n",
    "**解决方案：** 将奖励重新定义为只包含对比损失分子部分的负数：$r_t=-sim(x^1_t,x^2_t)$，再加上一个常数，确保奖励非负<br>\n",
    "**有效性：** 这个奖励只关心正本对之间的一致性。如果策略静止不动，所有正样本对的相似度会非常高，导致奖励变低。因此，为了获得高奖励，策略必须去寻找那些即使经过数据增强，其表征也难以保持一致的图像，这就促进了视觉上多样和新奇的观测\n",
    "2. 训练稳定化技巧\n",
    "**奖励归一化：** 对奖励进行归一化，防止奖励尺度的剧烈变化\n",
    "**网络架构调整：** 将ResNet中的批归一化层替换为组归一化层。因为批归一化再RL的在线学习环境中效果不佳。\n",
    "3. 优化算法\n",
    "**近端优化策略(PPO)：** 使用PPO训练$\\pi_\\theta$\n"
   ],
   "id": "21a3b66d64a3d0d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T05:37:39.148943Z",
     "start_time": "2025-10-16T05:37:39.146451Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "adf07744c9f4ef8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "239e9bdaf16c4d1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
